export SingleNNModel, MultiNNModel, constructNNModel, prepare_hidden_chain, RecurrenceOutputDense

using Lux, LuxCore
using ..EasyHybrid: hard_sigmoid

# Pure Neural Network Models (no mechanistic component)

struct SingleNNModel <: LuxCore.AbstractLuxContainerLayer{
        (
            :NN, :predictors, :targets, :scale_nn_outputs,
        ),
    }
    NN::Chain
    predictors::Vector{Symbol}
    targets::Vector{Symbol}
    scale_nn_outputs::Bool
end

"""
    RecurrenceOutputDense(in_dims => out_dims, [activation])

A layer that wraps a Dense layer to handle sequence outputs from Recurrence layers.

When a Recurrence layer has `return_sequence=true`, it outputs a tuple/vector of arrays 
(one per timestep). This layer broadcasts the Dense operation over each timestep and 
reshapes the result to `(features, timesteps, batch)` format.

# Arguments
- `in_dims::Int`: Input dimension (should match Recurrence output dimension)
- `out_dims::Int`: Output dimension
- `activation`: Activation function (default: `identity`)

# Example
```julia
# Instead of manually creating:
broadcast_layer = @compact(; layer = Dense(15 => 15)) do x
    y = map(layer, x)
    @return permutedims(stack(y; dims = 3), (1, 3, 2))
end

# Simply use:
Chain(
    Recurrence(LSTMCell(15 => 15), return_sequence = true),
    RecurrenceOutputDense(15 => 15)
)
```
"""
struct RecurrenceOutputDense{D <: Dense} <: LuxCore.AbstractLuxWrapperLayer{:layer}
    layer::D
end

function RecurrenceOutputDense(mapping::Pair{Int, Int}, activation = identity)
    return RecurrenceOutputDense(Dense(mapping.first, mapping.second, activation))
end

function RecurrenceOutputDense(in_dims::Int, out_dims::Int, activation = identity)
    return RecurrenceOutputDense(Dense(in_dims, out_dims, activation))
end

# Handle tuple output from Recurrence (return_sequence = true)
function (m::RecurrenceOutputDense)(x::NTuple{N, <:AbstractArray}, ps, st) where {N}
    y = map(xi -> first(LuxCore.apply(m.layer, xi, ps, st)), x)
    result = permutedims(stack(y; dims = 3), (1, 3, 2))
    return result, st
end

# Handle vector output from Recurrence (return_sequence = true)
function (m::RecurrenceOutputDense)(x::AbstractVector{<:AbstractArray}, ps, st)
    y = map(xi -> first(LuxCore.apply(m.layer, xi, ps, st)), x)
    result = permutedims(stack(y; dims = 3), (1, 3, 2))
    return result, st
end

# Fallback for regular array input (non-sequence mode)
function (m::RecurrenceOutputDense)(x::AbstractArray, ps, st)
    return LuxCore.apply(m.layer, x, ps, st)
end

"""
    prepare_hidden_chain(hidden_layers, in_dim, out_dim; activation, input_batchnorm=false)

Construct a neural network `Chain` for use in NN models.

# Arguments
- `hidden_layers::Union{Vector{Int}, Chain}`: 
    - If a `Vector{Int}`, specifies the sizes of each hidden layer. 
      For example, `[32, 16]` creates two hidden layers with 32 and 16 units, respectively.
    - If a `Chain`, the user provides a pre-built chain of hidden layers (excluding input/output layers).
      If the chain ends with a `Recurrence` layer, a `RecurrenceOutputDense` layer is automatically
      added to handle the sequence output format.
- `in_dim::Int`: Number of input features (input dimension).
- `out_dim::Int`: Number of output features (output dimension).
- `activation`: Activation function to use in hidden layers (default: `tanh`).
- `input_batchnorm::Bool`: If `true`, applies a `BatchNorm` layer to the input (default: `false`).

# Returns
- A `Chain` object representing the full neural network, with the following structure:
    - Optional input batch normalization (if `input_batchnorm=true`)
    - Input layer: `Dense(in_dim, h₁, activation)` where `h₁` is the first hidden size
    - Hidden layers: either user-supplied `Chain` or constructed from `hidden_layers`
    - If last hidden layer is a `Recurrence`, a `RecurrenceOutputDense` is added to handle sequence output
    - Output layer: `Dense(hₖ, out_dim)` where `hₖ` is the last hidden size

where `h₁` is the first hidden size and `hₖ` the last.

# Example with Recurrence (LSTM)
```julia
# User only needs to define:
NN_Memory = Chain(
    Recurrence(LSTMCell(15 => 15), return_sequence = true),
)

# The function automatically adds the RecurrenceOutputDense layer to handle sequence output
model = constructHybridModel(..., hidden_layers = NN_Memory, ...)
```
"""
function prepare_hidden_chain(
        hidden_layers::Union{Vector{Int}, Chain},
        in_dim::Int,
        out_dim::Int;
        activation = tanh,
        input_batchnorm = false # apply batchnorm to input as an easy way for normalization
    )
    if hidden_layers isa Chain
        # user gave a chain of hidden layers only

        # Helper to safely extract dimensions from layers
        function get_layer_dim(l, type)
            if type == :input
                hasproperty(l, :in_dims) && return l.in_dims
                (l isa BatchNorm && hasproperty(l, :dims)) && return l.dims
                (l isa Recurrence && hasproperty(l.cell, :in_dims)) && return l.cell.in_dims
                (l isa CompactLuxLayer && hasproperty(l.layers, :in_dims)) && return l.layers.in_dims
            elseif type == :output
                hasproperty(l, :out_dims) && return l.out_dims
                (l isa BatchNorm && hasproperty(l, :dims)) && return l.dims
                (l isa Recurrence && hasproperty(l.cell, :out_dims)) && return l.cell.out_dims
                (l isa CompactLuxLayer && hasproperty(l.layers, :out_dims)) && return l.layers.out_dims
            end
            return nothing
        end

        # Check if last layer is a Recurrence layer (needs special handling for sequence output)
        # In this framework, we ALWAYS assume return_sequence=true for Recurrence layers
        # (this is the EasyHybrid convention, regardless of Lux's default)
        function is_sequence_recurrence(layer)
            return layer isa Recurrence
        end

        last_layer = hidden_layers.layers[end]
        ends_with_sequence_recurrence = is_sequence_recurrence(last_layer)

        # Determine first_h by searching forward
        first_h = nothing
        for i in 1:length(hidden_layers)
            d = get_layer_dim(hidden_layers[i], :input)
            if !isnothing(d)
                first_h = d
                break
            end
        end
        isnothing(first_h) && error("Could not determine input dimension of hidden_layers Chain.")

        # Determine last_h by searching backward
        last_h = nothing
        for i in length(hidden_layers):-1:1
            d = get_layer_dim(hidden_layers[i], :output)
            if !isnothing(d)
                last_h = d
                break
            end
        end
        isnothing(last_h) && error("Could not determine output dimension of hidden_layers Chain.")

        if ends_with_sequence_recurrence
            # Chain ends with Recurrence layer (return_sequence=true) - add RecurrenceOutputDense to handle sequence output
            return Chain(
                input_batchnorm ? BatchNorm(in_dim, affine = false) : identity,
                Dense(in_dim, first_h, activation),
                hidden_layers.layers...,
                RecurrenceOutputDense(last_h => last_h, activation),
                Dense(last_h, out_dim)
            )
        else
            return Chain(
                input_batchnorm ? BatchNorm(in_dim, affine = false) : identity,
                Dense(in_dim, first_h, activation),
                hidden_layers.layers...,
                Dense(last_h, out_dim)
            )
        end
    else
        # user gave a vector of hidden‐layer sizes
        hs = hidden_layers
        isempty(hs) && return Chain()
        in_dim == 0 && return Chain()
        return Chain(
            input_batchnorm ? BatchNorm(in_dim, affine = false) : identity,
            Dense(in_dim, hs[1], activation),
            (Dense(hs[i], hs[i + 1], activation) for i in 1:(length(hs) - 1))...,
            Dense(hs[end], out_dim)
        )
    end
end

"""
    constructNNModel(predictors, targets; hidden_layers, activation, scale_nn_outputs)

Main constructor: `hidden_layers` can be either
  • a `Vector{Int}` of sizes, or
  • a `Chain` of hidden-layer `Dense` blocks.
"""
function constructNNModel(
        predictors::Vector{Symbol},
        targets::Vector{Symbol};
        hidden_layers::Union{Vector{Int}, Chain} = [32, 16, 16],
        activation = tanh,
        scale_nn_outputs::Bool = true,
        input_batchnorm = false
    )
    in_dim = length(predictors)
    out_dim = length(targets)

    NN = prepare_hidden_chain(
        hidden_layers, in_dim, out_dim;
        activation = activation,
        input_batchnorm = input_batchnorm
    )

    return SingleNNModel(NN, predictors, targets, scale_nn_outputs)
end

# MultiNNModel remains as before
struct MultiNNModel <: LuxCore.AbstractLuxContainerLayer{
        (
            :NNs, :predictors, :targets, :scale_nn_outputs,
        ),
    }
    NNs::NamedTuple
    predictors::NamedTuple
    targets::Vector{Symbol}
    scale_nn_outputs::Bool
end

function constructNNModel(
        predictors::NamedTuple,
        targets;
        scale_nn_outputs = true
    )
    @assert collect(keys(predictors)) == targets "predictor names must match targets"
    NNs = NamedTuple()
    for (nn_name, preds) in pairs(predictors)
        nn = Chain(
            BatchNorm(length(preds), affine = false),
            Dense(length(preds), 15, sigmoid),
            Dense(15, 15, sigmoid),
            Dense(15, 1, x -> x^2)
        )
        NNs = merge(NNs, NamedTuple{(nn_name,), Tuple{typeof(nn)}}((nn,)))
    end
    return MultiNNModel(NNs, predictors, targets, scale_nn_outputs)
end

# LuxCore initial parameters for SingleNNModel
function LuxCore.initialparameters(rng::AbstractRNG, m::SingleNNModel)
    ps_nn, _ = LuxCore.setup(rng, m.NN)
    nt = (; ps = ps_nn)
    return nt
end

# LuxCore initial parameters for MultiNNModel
function LuxCore.initialparameters(rng::AbstractRNG, m::MultiNNModel)
    nn_params = NamedTuple()
    for (nn_name, nn) in pairs(m.NNs)
        ps_nn, _ = LuxCore.setup(rng, nn)
        nn_params = merge(nn_params, NamedTuple{(nn_name,), Tuple{typeof(ps_nn)}}((ps_nn,)))
    end
    nt = (; nn_params...)
    return nt
end

# LuxCore initial states for SingleNNModel
function LuxCore.initialstates(rng::AbstractRNG, m::SingleNNModel)
    _, st_nn = LuxCore.setup(rng, m.NN)
    nt = (; st_nn = st_nn)
    return nt
end

# LuxCore initial states for MultiNNModel
function LuxCore.initialstates(rng::AbstractRNG, m::MultiNNModel)
    nn_states = NamedTuple()
    for (nn_name, nn) in pairs(m.NNs)
        _, st_nn = LuxCore.setup(rng, nn)
        nn_states = merge(nn_states, NamedTuple{(nn_name,), Tuple{typeof(st_nn)}}((st_nn,)))
    end
    nt = (; nn_states...)
    return nt
end

# Forward pass for SingleNNModel
function (m::SingleNNModel)(ds_k, ps, st)
    predictors = toArray(ds_k, m.predictors)
    nn_out, st_nn = LuxCore.apply(m.NN, predictors, ps.ps, st.st_nn)
    nn_cols = eachrow(nn_out)
    nn_params = NamedTuple(zip(m.targets, nn_cols))
    if m.scale_nn_outputs
        scaled_nn_vals = Tuple(hard_sigmoid(nn_params[name]) for name in m.targets)
    else
        scaled_nn_vals = Tuple(nn_params[name] for name in m.targets)
    end
    scaled_nn_params = NamedTuple(zip(m.targets, scaled_nn_vals))

    out = (; scaled_nn_params...)
    st_new = (; st_nn = st_nn)
    return out, st_new
end

# Forward pass for MultiNNModel
function (m::MultiNNModel)(ds_k, ps, st)
    nn_inputs = NamedTuple()
    for (nn_name, predictors) in pairs(m.predictors)
        da = toArray(ds_k, predictors)
        nn_inputs = merge(nn_inputs, NamedTuple{(nn_name,), Tuple{typeof(da)}}((da,)))
    end
    nn_outputs = NamedTuple()
    nn_states = NamedTuple()
    for (nn_name, nn) in pairs(m.NNs)
        nn_out, st_nn = LuxCore.apply(nn, nn_inputs[nn_name], ps[nn_name], st[nn_name])
        nn_outputs = merge(nn_outputs, NamedTuple{(nn_name,), Tuple{typeof(nn_out)}}((nn_out,)))
        nn_states = merge(nn_states, NamedTuple{(nn_name,), Tuple{typeof(st_nn)}}((st_nn,)))
    end
    scaled_nn_params = NamedTuple()
    for (nn_name, target_name) in zip(keys(m.NNs), m.targets)
        nn_output = nn_outputs[nn_name]
        nn_cols = eachrow(nn_output)
        nn_param = NamedTuple{(target_name,), Tuple{typeof(nn_cols[1])}}((nn_cols[1],))
        if m.scale_nn_outputs
            scaled_nn_val = hard_sigmoid(nn_param[target_name])
        else
            scaled_nn_val = nn_param[target_name]
        end
        nn_scaled_param = NamedTuple{(target_name,), Tuple{typeof(scaled_nn_val)}}((scaled_nn_val,))
        scaled_nn_params = merge(scaled_nn_params, nn_scaled_param)
    end
    out = (; scaled_nn_params..., nn_outputs = nn_outputs)
    st_new = (; nn_states...)
    return out, st_new
end

# Display functions
function Base.show(io::IO, ::MIME"text/plain", m::SingleNNModel)
    println("Neural Network: ", m.NN)
    println("Predictors: ", m.predictors)
    return println("scale NN outputs: ", m.scale_nn_outputs)
end

function Base.show(io::IO, ::MIME"text/plain", m::MultiNNModel)
    println("Neural Networks:")
    for (name, nn) in pairs(m.NNs)
        println("  $name: ", nn)
    end
    println("Predictors:")
    for (name, preds) in pairs(m.predictors)
        println("  $name: ", preds)
    end
    return println("scale NN outputs: ", m.scale_nn_outputs)
end

struct BroadcastLayer{T <: NamedTuple} <: LuxCore.AbstractLuxContainerLayer{(:layers,)}
    layers::T
end

function BroadcastLayer(layers...)
    for l in layers
        if !iszero(LuxCore.statelength(l))
            throw(ArgumentError("Stateful layer `$l` are not supported for `BroadcastLayer`."))
        end
    end
    names = ntuple(i -> Symbol("layer_$i"), length(layers))
    return BroadcastLayer(NamedTuple{names}(layers))
end

BroadcastLayer(; kwargs...) = BroadcastLayer(connection, (; kwargs...))

function (m::BroadcastLayer)(x, ps, st::NamedTuple{names}) where {names}
    results = (first ∘ Lux.apply).(values(m.layers), x, values(ps), values(st))
    return results, st
end

Base.keys(m::BroadcastLayer) = Base.keys(getfield(m, :layers))
