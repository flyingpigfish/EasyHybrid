export train, TrainResults, prepare_data, split_data, split_into_sequences
# beneficial for plotting based on type TrainResults?
struct TrainResults
    train_history
    val_history
    ps_history
    train_obs_pred
    val_obs_pred
    train_diffs
    val_diffs
    ps
    st
    best_epoch
    best_loss
end

"""
    train(hybridModel, data, save_ps; nepochs=200, batchsize=10, opt=Adam(0.01), patience=typemax(Int),
          file_name=nothing, loss_types=[:mse, :r2], training_loss=:mse, agg=sum, train_from=nothing,
          random_seed=161803, yscale=log10, monitor_names=[], return_model=:best, 
          plotting=true, show_progress=true, hybrid_name=randstring(10), kwargs...)

Train a hybrid model using the provided data and save the training process to a file in JLD2 format. 
Default output file is `trained_model.jld2` at the current working directory under `output_tmp`.

# Arguments:
- `hybridModel`: The hybrid model to be trained.
- `data`: The training data, either a single DataFrame, a single KeyedArray, or a tuple of KeyedArrays.
- `save_ps`: A tuple of physical parameters to save during training.

## Core Training Parameters:
- `nepochs`: Number of training epochs (default: 200).
- `batchsize`: Size of the training batches (default: 10).
- `opt`: The optimizer to use for training (default: Adam(0.01)).
- `patience`: The number of epochs to wait before early stopping (default: `typemax(Int)` -> no early stopping).

## Loss and Evaluation:
- `training_loss`: The loss type to use during training (default: `:mse`).
- `loss_types`: A vector of loss types to compute during training (default: `[:mse, :r2]`). The first entry is used for plotting in the dynamic trainboard. This loss can be increasing (e.g. NSE) or decreasing (e.g. RMSE).
- `agg`: The aggregation function to apply to the computed losses (default: `sum`).

## Data Handling:
- `array_type`: Array type for data conversion from DataFrame: `:DimArray` (default) or `:KeyedArray`.
- `shuffleobs`: Whether to shuffle the training data (default: false).
- `split_by_id`: Column name or function to split data by ID (default: nothing -> no ID-based splitting).
- `split_data_at`: Fraction of data to use for training when splitting (default: 0.8).
- `folds`: Vector or column name of fold assignments (1..k), one per sample/column for k-fold cross-validation (default: nothing).
- `val_fold`: The validation fold to use when `folds` is provided (default: nothing).

## Training State and Reproducibility:
- `train_from`: A tuple of physical parameters and state to start training from or an output of `train` (default: nothing -> new training).
- `random_seed`: The random seed to use for training (default: 161803).

## Output and Monitoring:
- `file_name`: The name of the file to save the training process (default: nothing -> "trained_model.jld2").
- `hybrid_name`: Name identifier for the hybrid model (default: randomly generated 10-character string).
- `return_model`: The model to return: `:best` for the best model, `:final` for the final model (default: `:best`).
- `monitor_names`: A vector of monitor names to track during training (default: `[]`).
- `folder_to_save`: Additional folder name string to append to output path (default: "").

## Visualization and UI:
- `plotting`: Whether to generate plots during training (default: true).
- `show_progress`: Whether to show progress bars during training (default: true).
- `yscale`: The scale to apply to the y-axis for plotting (default: `log10`).
"""
function train(
        hybridModel, data, save_ps;
        # Core training parameters
        nepochs = 200,
        batchsize = 64,
        opt = Adam(0.01),
        patience = typemax(Int),
        autodiff_backend = AutoZygote(),
        return_gradients = True(),
        # Array type for data conversion
        array_type = :KeyedArray,  # :DimArray or :KeyedArray
        # Loss and evaluation
        training_loss = :mse,
        loss_types = [:mse, :r2],
        extra_loss = nothing,
        agg = sum,

        # Data handling parameters are now passed via kwargs...

        # Training state and reproducibility
        train_from = nothing,
        random_seed = 161803,

        # Output and monitoring
        file_name = nothing,
        hybrid_name = "",
        return_model = :best,
        monitor_names = [],
        folder_to_save = "",

        # Visualization and UI
        plotting = true,
        show_progress = true,
        yscale = identity,
        kwargs...
    )

    #! check if the EasyHybridMakie extension is loaded.
    ext = Base.get_extension(@__MODULE__, :EasyHybridMakie)

    #! check if the training loss is a minimizing loss.
    check_training_loss(training_loss)

    if ext === nothing
        @warn "Makie extension not loaded, no plots will be generated."
    end

    if !plotting
        ext = nothing
        @info "Plotting disabled."
    end

    if !isnothing(random_seed)
        Random.seed!(random_seed)
    end

    (x_train, y_train), (x_val, y_val) = split_data(data, hybridModel; array_type = array_type, kwargs...)

    train_loader = DataLoader((x_train, y_train), batchsize = batchsize, shuffle = true)

    @info "Training data type: $(typeof(x_train))"

    if isnothing(train_from)
        ps, st = LuxCore.setup(Random.default_rng(), hybridModel)
    else
        ps, st = get_ps_st(train_from)
    end
    ps = ComponentArray(ps)
    train_state = Lux.Training.TrainState(hybridModel, ps, st, opt)

    # ? initial losses
    is_no_nan_t = .!isnan.(y_train)
    is_no_nan_v = .!isnan.(y_val)

    eval_metric = loss_types[1]

    l_init_train, _, init_ŷ_train = evaluate_acc(hybridModel, x_train, y_train, is_no_nan_t, ps, st, loss_types, training_loss, extra_loss, agg)
    l_init_val, _, init_ŷ_val = evaluate_acc(hybridModel, x_val, y_val, is_no_nan_v, ps, st, loss_types, training_loss, extra_loss, agg)

    train_history = [l_init_train]
    val_history = [l_init_val]
    target_names = hybridModel.targets
    fig = nothing
    # Initialize plotting observables if extension is loaded
    if !isnothing(ext)
        init_observables, fixed_observations = initialize_plotting_observables(
            init_ŷ_train,
            init_ŷ_val,
            y_train,
            y_val,
            l_init_train,
            l_init_val,
            eval_metric,
            agg,
            target_names;
            monitor_names
        )
        zoom_epochs = min(patience, 50)
        # ! Launch dashboard if extension is loaded
        EasyHybrid.train_board(init_observables..., fixed_observations..., yscale, target_names, string(eval_metric); monitor_names, zoom_epochs)
        fig = EasyHybrid.dashboard_figure()
    end

    # track physical parameters
    ps_values_init = [copy(getproperty(ps, e)[1]) for e in save_ps]
    ps_init = NamedTuple{save_ps}(ps_values_init)

    # output also monitored names
    init_monitor_train_values = [vec(getfield(init_ŷ_train, m)) for m in monitor_names]
    ps_monitor_train = NamedTuple{Tuple(monitor_names)}(init_monitor_train_values)
    init_monitor_val_values = [vec(getfield(init_ŷ_val, m)) for m in monitor_names]
    ps_monitor_val = NamedTuple{Tuple(monitor_names)}(init_monitor_val_values)

    ps_history = [(; ϕ = ps_init, monitor = (; train = ps_monitor_train, val = ps_monitor_val))]


    # For Early stopping
    best_ps = deepcopy(ps)
    best_st = deepcopy(st)
    best_loss = l_init_val
    best_epoch = 0
    cnt_patience = 0

    # Initialize best_agg_loss for early stopping comparison based on the first loss_types in [:mse, :r2]
    best_agg_loss = getproperty(l_init_val[1], Symbol(agg))
    val_metric_name = first(keys(l_init_val))
    current_agg_loss = best_agg_loss  # Initialize for potential use in final logging

    file_name = resolve_path(file_name; folder_to_save)
    save_ps_st(file_name, hybridModel, ps, st, save_ps)
    suffix = hybrid_name == "" ? "" : "_" * hybrid_name
    file_name_best = resolve_path("best_model$(suffix).jld2"; folder_to_save)
    save_ps_st(file_name_best, hybridModel, ps, st, save_ps)

    save_train_val_loss!(file_name, l_init_train, "training_loss", 0)
    save_train_val_loss!(file_name, l_init_val, "validation_loss", 0)

    # save/record
    tmp_folder = get_output_path(; folder_to_save)
    @info "Check the saved output (.png, .mp4, .jld2) from training at: $(tmp_folder)"

    prog = Progress(nepochs, desc = "Training loss", enabled = show_progress)
    loss(hybridModel, ps, st, (x, y)) = compute_loss(
        hybridModel, ps, st, (x, y);
        logging = LoggingLoss(train_mode = true, loss_types = loss_types, training_loss = training_loss, extra_loss = extra_loss, agg = agg)
    )
    maybe_record_history(!isnothing(ext), fig, joinpath(tmp_folder, "training_history_$(hybrid_name).mp4"); framerate = 24) do io
        for epoch in 1:nepochs
            for (x, y) in train_loader
                # ? check NaN indices before going forward, and pass filtered `x, y`.
                is_no_nan = .!isnan.(y)
                if length(is_no_nan) > 0 # ! be careful here, multivariate needs fine tuning
                    # ? let's keep grads, they might be useful for mixed gradient methods
                    grads, loss_val, stats, train_state = Lux.Training.single_train_step!(
                        autodiff_backend, loss, (x, (y, is_no_nan)), train_state;
                        return_gradients
                    )
                end
            end

            # sync ps and st from train_state
            ps = train_state.parameters
            st = train_state.states

            ps_values = [copy(getproperty(ps, e)[1]) for e in save_ps]
            tmp_e = NamedTuple{save_ps}(ps_values)

            l_train, _, current_ŷ_train = evaluate_acc(hybridModel, x_train, y_train, is_no_nan_t, ps, st, loss_types, training_loss, extra_loss, agg)
            l_val, _, current_ŷ_val = evaluate_acc(hybridModel, x_val, y_val, is_no_nan_v, ps, st, loss_types, training_loss, extra_loss, agg)
            # save also monitored names
            current_monitor_train_values = [vec(getfield(current_ŷ_train, m)) for m in monitor_names]
            c_ps_monitor_train = NamedTuple{Tuple(monitor_names)}(current_monitor_train_values)
            current_monitor_val_values = [vec(getfield(current_ŷ_val, m)) for m in monitor_names]
            c_ps_monitor_val = NamedTuple{Tuple(monitor_names)}(current_monitor_val_values)

            tmp_history = (; ϕ = tmp_e, monitor = (; train = c_ps_monitor_train, val = c_ps_monitor_val))

            push!(ps_history, tmp_history)

            save_ps_st!(file_name, hybridModel, ps, st, save_ps, epoch)
            save_train_val_loss!(file_name, l_train, "training_loss", epoch)
            save_train_val_loss!(file_name, l_val, "validation_loss", epoch)

            push!(train_history, l_train)
            push!(val_history, l_val)

            # Update plotting observables if extension is loaded
            if !isnothing(ext)
                update_plotting_observables(
                    init_observables...,
                    l_train,
                    l_val,
                    eval_metric,
                    agg,
                    current_ŷ_train,
                    current_ŷ_val,
                    target_names,
                    epoch;
                    monitor_names
                )
                # record a new frame
                recordframe!(io)
            end

            current_agg_loss = getproperty(l_val[1], Symbol(agg))

            if isbetter(current_agg_loss, best_agg_loss, eval_metric)
                best_agg_loss = current_agg_loss
                best_ps = deepcopy(ps)
                best_st = deepcopy(st)
                cnt_patience = 0
                best_epoch = epoch
            else
                cnt_patience += 1
            end
            if cnt_patience >= patience
                metric_name = first(keys(l_val))
                if !isnothing(ext)
                    img_name = joinpath(tmp_folder, "train_history_best_epoch_$(best_epoch).png")
                    save_fig(img_name, dashboard_figure())
                    img_name = joinpath(tmp_folder, "train_history_$(hybrid_name).png")
                    save_fig(img_name, dashboard_figure())
                end
                @warn "Early stopping at epoch $epoch with best validation loss wrt $metric_name: $best_agg_loss"
                break
            end

            if !isnothing(ext) && epoch == nepochs
                img_name = joinpath(tmp_folder, "train_history_best_epoch_$(best_epoch).png")
                save_fig(img_name, dashboard_figure())
                img_name = joinpath(tmp_folder, "train_history_$(hybrid_name).png")
                save_fig(img_name, dashboard_figure())
            end

            _headers, paddings = header_and_paddings(get_loss_entries(l_init_train, eval_metric))

            next!(
                prog; showvalues = [
                    ("epoch ", epoch),
                    ("targets ", join(_headers, "  ")),
                    (styled"{red:training-start }", styled_values(get_loss_entries(l_init_train, eval_metric); paddings)),
                    (styled"{bright_red:current }", styled_values(get_loss_entries(l_train, eval_metric); color = :bright_red, paddings)),
                    (styled"{cyan:validation-start }", styled_values(get_loss_entries(l_init_val, eval_metric); paddings)),
                    (styled"{bright_cyan:current }", styled_values(get_loss_entries(l_val, eval_metric); color = :bright_cyan, paddings)),
                ]
            )
            # TODO: log metrics
        end
    end

    train_history = WrappedTuples(train_history)
    val_history = WrappedTuples(val_history)
    ps_history = WrappedTuples(ps_history)

    # ? save final evaluation or best at best validation value
    if return_model == :best
        ps, st = deepcopy(best_ps), deepcopy(best_st)
        @info "Returning best model from epoch $best_epoch of $nepochs epochs with best validation loss wrt $val_metric_name: $best_agg_loss"
        save_epoch = best_epoch == 0 ? 1 : best_epoch
        save_ps_st!(file_name_best, hybridModel, ps, st, save_ps, save_epoch)
    elseif return_model == :final
        ps, st = deepcopy(ps), deepcopy(st)
        @info "Returning final model from final of $nepochs epochs with validation loss: $current_agg_loss, the best validation loss was $best_agg_loss from epoch $best_epoch wrt $val_metric_name"
    else
        @warn "Invalid return_model: $return_model. Returning final model."
    end

    ŷ_train, αst_train = hybridModel(x_train, ps, LuxCore.testmode(st))
    ŷ_val, αst_val = hybridModel(x_val, ps, LuxCore.testmode(st))
    save_predictions!(file_name, ŷ_train, αst_train, "training")
    save_predictions!(file_name, ŷ_val, αst_val, "validation")

    # training
    save_observations!(file_name, target_names, y_train, "training")
    save_observations!(file_name, target_names, y_val, "validation")
    # save split obs (targets)

    # ? this could be saved to disk if needed for big sizes.
    train_obs = toDataFrame(y_train)
    train_hats = toDataFrame(ŷ_train, target_names)
    train_obs_pred = hcat(train_obs, train_hats)
    # validation
    val_obs = toDataFrame(y_val)
    val_hats = toDataFrame(ŷ_val, target_names)
    val_obs_pred = hcat(val_obs, val_hats)
    # ? diffs, additional predictions without observational counterparts!
    # TODO: better!
    set_diff = setdiff(keys(ŷ_train), target_names)
    train_diffs = !isempty(set_diff) ? NamedTuple{Tuple(set_diff)}([getproperty(ŷ_train, e) for e in set_diff]) : nothing
    val_diffs = !isempty(set_diff) ? NamedTuple{Tuple(set_diff)}([getproperty(ŷ_val, e) for e in set_diff]) : nothing

    # TODO: save/output metrics
    return TrainResults(
        train_history,
        val_history,
        ps_history,
        train_obs_pred,
        val_obs_pred,
        train_diffs,
        val_diffs,
        ps,
        st,
        best_epoch,
        best_agg_loss
    )
end

function evaluate_acc(ghm, x, y, y_no_nan, ps, st, loss_types, training_loss, extra_loss, agg)
    loss_val, sts, ŷ = compute_loss(ghm, ps, st, (x, (y, y_no_nan)), logging = LoggingLoss(train_mode = false, loss_types = loss_types, training_loss = training_loss, extra_loss = extra_loss, agg = agg))
    return loss_val, sts, ŷ
end
function maybe_record_history(block, should_record, fig, output_path; framerate = 24)
    return if should_record
        record_history(fig, output_path; framerate = framerate) do io
            block(io)
        end
    else
        block(nothing)  # call with dummy io
    end
end

function styled_values(nt; digits = 5, color = nothing, paddings = nothing)
    formatted = [
        begin
                value_str = @sprintf("%.*f", digits, v)
                padded = isnothing(paddings) ? value_str : rpad(value_str, paddings[i])
                isnothing(color) ? padded : styled"{$color:$padded}"
            end
            for (i, v) in enumerate(values(nt))
    ]
    return join(formatted, "  ")
end

function header_and_paddings(nt; digits = 5)
    min_val_width = digits + 2  # 1 for "0", 1 for ".", rest for digits
    paddings = map(k -> max(length(string(k)), min_val_width), keys(nt))
    headers = [rpad(string(k), w) for (k, w) in zip(keys(nt), paddings)]
    return headers, paddings
end

function split_data(data::Tuple{Tuple, Tuple}, hybridModel; kwargs...)
    @warn "data was prepared already, none of the keyword arguments for split_data will be used"
    return data
end

function split_data(
        data::Union{DataFrame, KeyedArray, Tuple, AbstractDimArray},
        hybridModel;
        split_by_id::Union{Nothing, Symbol, AbstractVector} = nothing,
        folds::Union{Nothing, AbstractVector, Symbol} = nothing,
        val_fold::Union{Nothing, Int} = nothing,
        shuffleobs::Bool = false,
        split_data_at::Real = 0.8,
        sequence_kwargs::Union{Nothing, NamedTuple} = nothing,
        array_type::Symbol = :KeyedArray,
        kwargs...
    )
    data_ = prepare_data(hybridModel, data; array_type = array_type)

    if sequence_kwargs !== nothing
        x_keyed, y_keyed = data_
        sis_default = (; input_window = 10, output_window = 1, output_shift = 1, lead_time = 1)
        sis = merge(sis_default, sequence_kwargs)
        @info "Using split_into_sequences: $sis"
        x_all, y_all = split_into_sequences(x_keyed, y_keyed; sis.input_window, sis.output_window, sis.output_shift, sis.lead_time)
    else
        x_all, y_all = data_
    end


    if split_by_id !== nothing && folds !== nothing

        throw(ArgumentError("split_by_id and folds are not supported together; do the split when constructing folds"))

    elseif split_by_id !== nothing
        # --- Option A: split by ID ---
        ids = isa(split_by_id, Symbol) ? getbyname(data, split_by_id) : split_by_id
        unique_ids = unique(ids)
        train_ids, val_ids = splitobs(unique_ids; at = split_data_at, shuffle = shuffleobs)
        train_idx = findall(in(train_ids), ids)
        val_idx = findall(in(val_ids), ids)

        @info "Splitting data by $(split_by_id)"
        @info "Number of unique $(split_by_id): $(length(unique_ids))"
        @info "Train IDs: $(length(train_ids)) | Val IDs: $(length(val_ids))"

        x_train, y_train = view_end_dim(x_all, train_idx), view_end_dim(y_all, train_idx)
        x_val, y_val = view_end_dim(x_all, val_idx), view_end_dim(y_all, val_idx)
        return (x_train, y_train), (x_val, y_val)

    elseif folds !== nothing || val_fold !== nothing
        # --- Option B: external K-fold assignment ---
        @assert val_fold !== nothing "Provide val_fold when using folds."
        @assert folds !== nothing "Provide folds when using val_fold."
        @warn "shuffleobs is not supported when using folds and val_fold, this will be ignored and should be done during fold constructions"
        f = isa(folds, Symbol) ? getbyname(data, folds) : folds
        n = size(x_all, 2)
        @assert length(f) == n "length(folds) ($(length(f))) must equal number of samples/columns ($n)."
        @assert 1 ≤ val_fold ≤ maximum(f) "val_fold=$val_fold is out of range 1:$(maximum(f))."

        val_idx = findall(==(val_fold), f)
        @assert !isempty(val_idx) "No samples assigned to validation fold $val_fold."
        train_idx = setdiff(1:n, val_idx)

        @info "K-fold via external assignments: val_fold=$val_fold → train=$(length(train_idx)) val=$(length(val_idx))"

        x_train, y_train = view_end_dim(x_all, train_idx), view_end_dim(y_all, train_idx)
        x_val, y_val = view_end_dim(x_all, val_idx), view_end_dim(y_all, val_idx)
        return (x_train, y_train), (x_val, y_val)

    else
        # --- Fallback: simple random/chronological split of prepared data ---
        (x_train, y_train), (x_val, y_val) = splitobs((x_all, y_all); at = split_data_at, shuffle = shuffleobs)
        return (x_train, y_train), (x_val, y_val)
    end
end


"""
    split_data(data, hybridModel; split_by_id=nothing, shuffleobs=false, split_data_at=0.8, kwargs...)
    split_data(data::Union{DataFrame, KeyedArray}, hybridModel; split_by_id=nothing, shuffleobs=false, split_data_at=0.8, folds=nothing, val_fold=nothing, kwargs...)
    split_data(data::AbstractDimArray, hybridModel; split_by_id=nothing, shuffleobs=false, split_data_at=0.8, kwargs...)
    split_data(data::Tuple, hybridModel; split_by_id=nothing, shuffleobs=false, split_data_at=0.8, kwargs...)
    split_data(data::Tuple{Tuple, Tuple}, hybridModel; kwargs...)

Split data into training and validation sets, either randomly, by grouping by ID, or using external fold assignments.

# Arguments:
- `data`: The data to split, which can be a DataFrame, KeyedArray, AbstractDimArray, or Tuple
- `hybridModel`: The hybrid model object used for data preparation
- `split_by_id=nothing`: Either `nothing` for random splitting, a `Symbol` for column-based splitting, or an `AbstractVector` for custom ID-based splitting
- `shuffleobs=false`: Whether to shuffle observations during splitting
- `split_data_at=0.8`: Ratio of data to use for training
- `folds`: Vector or column name of fold assignments (1..k), one per sample/column for k-fold cross-validation
- `val_fold`: The validation fold to use when `folds` is provided

# Behavior:
- For DataFrame/KeyedArray: Supports random splitting, ID-based splitting, and external fold assignments
- For AbstractDimArray/Tuple: Random splitting only after data preparation
- For pre-split Tuple{Tuple, Tuple}: Returns input unchanged

# Returns:
- `((x_train, y_train), (x_val, y_val))`: Tuple containing training and validation data pairs
"""
function split_data end

function prepare_data(hm, data::KeyedArray; array_type = :KeyedArray)
    predictors_forcing, targets = get_prediction_target_names(hm)
    # KeyedArray: use () syntax for views that are differentiable
    return (data(predictors_forcing), data(targets))
end

function prepare_data(hm, data::AbstractDimArray; array_type = :DimArray)
    predictors_forcing, targets = get_prediction_target_names(hm)
    # DimArray: use [] syntax (copies, but differentiable)
    return (data[variable = At(predictors_forcing)], data[variable = At(targets)])
end

function prepare_data(hm, data::DataFrame; array_type = :KeyedArray)
    predictors_forcing, targets = get_prediction_target_names(hm)

    all_predictor_cols = unique(vcat(values(predictors_forcing)...))
    col_to_select = unique([all_predictor_cols; targets])

    # subset to only the cols we care about
    sdf = data[!, col_to_select]

    mapcols(col -> replace!(col, missing => NaN), sdf; cols = names(sdf, Union{Missing, Real}))

    # Separate predictor/forcing vs. target columns
    predforce_cols = setdiff(col_to_select, targets)

    # For each row, check if *any* predictor/forcing is missing
    mask_missing_predforce = map(row -> any(isnan, row), eachrow(sdf[:, predforce_cols]))

    # For each row, check if *at least one* target is present (i.e. not all missing)
    mask_at_least_one_target = map(row -> any(!isnan, row), eachrow(sdf[:, targets]))

    # Keep rows where predictors/forcings are *complete* AND there's some target present
    keep = .!mask_missing_predforce .& mask_at_least_one_target
    sdf = sdf[keep, col_to_select]

    # Convert to Float32 and to the specified array type
    if array_type == :KeyedArray
        ds = to_keyedArray(Float32.(sdf))
    else
        ds = to_dimArray(Float32.(sdf))
    end
    return prepare_data(hm, ds; array_type = array_type)
end

function prepare_data(hm, data::Tuple; array_type = :DimArray)
    return data
end

"""
    prepare_data(hm, data::DataFrame)
    prepare_data(hm, data::KeyedArray)
    prepare_data(hm, data::AbstractDimArray)
    prepare_data(hm, data::Tuple)

Prepare data for training by extracting predictor/forcing and target variables based on the hybrid model's configuration.

# Arguments:
- `hm`: The Hybrid Model
- `data`: The input data, which can be a DataFrame, KeyedArray, or DimensionalData array.

# Returns:
- If `data` is a DataFrame, KeyedArray returns a tuple of (predictors_forcing, targets) as KeyedArrays.
- If `data` is an AbstractDimArray returns a tuple of (predictors_forcing, targets) of AbstractDimArrays.
- If `data` is already a Tuple, it is returned as-is.
"""
function prepare_data end

"""
    get_prediction_target_names(hm)
Utility function to extract predictor/forcing and target names from a hybrid model.

# Arguments:
- `hm`: The Hybrid Model

Returns a tuple of (predictors_forcing, targets) names.
"""
function get_prediction_target_names(hm)
    targets = hm.targets
    predictors_forcing = Symbol[]
    for prop in propertynames(hm)
        if occursin("predictors", string(prop))
            val = getproperty(hm, prop)
            if isa(val, AbstractVector)
                append!(predictors_forcing, val)
            elseif isa(val, Union{NamedTuple, Tuple})
                append!(predictors_forcing, unique(vcat(values(val)...)))
            end
        end
    end
    for prop in propertynames(hm)
        if occursin("forcing", string(prop))
            val = getproperty(hm, prop)
            if isa(val, AbstractVector)
                append!(predictors_forcing, val)
            elseif isa(val, Union{Tuple, NamedTuple})
                append!(predictors_forcing, unique(vcat(values(val)...)))
            end
        end
    end
    predictors_forcing = unique(predictors_forcing)

    if isempty(predictors_forcing)
        @warn "Note that you don't have predictors or forcing variables."
    end
    if isempty(targets)
        @warn "Note that you don't have target names."
    end
    return predictors_forcing, targets
end

function get_ps_st(train_from::TrainResults)
    return train_from.ps, train_from.st
end

function get_ps_st(train_from::Tuple)
    return train_from
end

function getbyname(df::DataFrame, name::Symbol)
    return df[!, name]
end

function getbyname(ka::Union{KeyedArray, AbstractDimArray}, name::Symbol)
    return @view ka[variable = At(name)]
end

function split_into_sequences(x, y; input_window = 5, output_window = 1, output_shift = 1, lead_time = 1)
    ndims(x) == 2 || throw(ArgumentError("expected x to be (feature, time); got ndims(x) = $(ndims(x))"))
    ndims(y) == 2 || throw(ArgumentError("expected y to be (target, time); got ndims(y) = $(ndims(y))"))

    Lx, Ly = size(x, 2), size(y, 2)
    Lx == Ly || throw(ArgumentError("x and y must have same time length; got $Lx vs $Ly"))
    lead_time ≥ 0 || throw(ArgumentError("lead_time must be ≥ 0 (0 = instantaneous end)"))

    nfeat, ntarget = size(x, 1), size(y, 1)
    L = Lx

    featkeys = axiskeys(x, 1)
    timekeys = axiskeys(x, 2)
    targetkeys = axiskeys(y, 1)

    lead_start = lead_time - output_window + 1

    lag_keys = Symbol.(["x$(input_window + lead_time - 1)_to_x$(lag)" for lag in (input_window + lead_time - 1):-1:lead_time])
    lead_keys = Symbol.(["_y$(lead)" for lead in ((output_window - 1):-1:0)])
    lead_keys = Symbol.(lag_keys[(end - length(lead_keys) + 1):end], lead_keys)
    lag_keys[(end - length(lead_keys) + 1):end] .= lead_keys

    sx_min = max(1, 1 - (input_window + lead_time - output_window))
    sx_max = L - input_window - lead_time + 1
    sx_min <= sx_max || throw(ArgumentError("windows too long for series length"))

    sx_vals = collect(sx_min:output_shift:sx_max)
    num_samples = length(sx_vals)
    num_samples ≥ 1 || throw(ArgumentError("no samples with given output_shift/windows"))

    samplekeys = timekeys[sx_vals]

    Xd = zeros(Float32, nfeat, input_window, num_samples)
    Yd = zeros(Float32, ntarget, output_window, num_samples)

    @inbounds @views for (ii, sx) in enumerate(sx_vals)
        ex = sx + input_window - 1
        sy = ex + lead_start
        ey = ex + lead_time
        Xd[:, :, ii] .= x[:, sx:ex]
        Yd[:, :, ii] .= y[:, sy:ey]
    end
    if x isa KeyedArray
        Xk = KeyedArray(Xd; variable = featkeys, time = lag_keys, batch_size = samplekeys)
        Yk = KeyedArray(Yd; variable = targetkeys, time = lead_keys, batch_size = samplekeys)
        return Xk, Yk
    elseif x isa AbstractDimArray
        Xk = DimArray(Xd, (variable = featkeys, time = lag_keys, batch_size = samplekeys))
        Yk = DimArray(Yd, (variable = targetkeys, time = lead_keys, batch_size = samplekeys))
        return Xk, Yk
    else
        throw(ArgumentError("expected Xd to be KeyedArray or AbstractDimArray; got $(typeof(Xd))"))
    end
end


function view_end_dim(x_all::Union{KeyedArray{Float32, 2}, AbstractDimArray{Float32, 2}}, idx)
    return view(x_all, :, idx)
end

function view_end_dim(x_all::Union{KeyedArray{Float32, 3}, AbstractDimArray{Float32, 3}}, idx)
    return view(x_all, :, :, idx)
end
